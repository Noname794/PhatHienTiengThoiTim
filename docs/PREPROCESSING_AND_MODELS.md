# Data Preprocessing v√† Deep Learning Models

## T·ªïng quan

Project n√†y bao g·ªìm:
1. **Module ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu** (`data_preprocessing.py`)
2. **CNN Model** (file g·ªëc: `04_training_cnn_method1 - Copy.ipynb`)
3. **LSTM/GRU Model** (file m·ªõi: `05_training_lstm_method.ipynb`)

---

## 1. Module Ti·ªÅn X·ª≠ L√Ω (`src/data_preprocessing.py`)

### M·ª•c ƒë√≠ch
T√°ch ri√™ng c√°c b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu ra m·ªôt module ƒë·ªôc l·∫≠p ƒë·ªÉ:
- T√°i s·ª≠ d·ª•ng cho nhi·ªÅu models kh√°c nhau
- D·ªÖ b·∫£o tr√¨ v√† c·∫≠p nh·∫≠t
- Code s·∫°ch h∆°n, tr√°nh duplicate

### Class: `HeartSoundPreprocessor`

#### Kh·ªüi t·∫°o
```python
from data_preprocessing import HeartSoundPreprocessor

preprocessor = HeartSoundPreprocessor(
    sr=4000,           # Sampling rate (Hz)
    max_len=3000,      # ƒê·ªô d√†i cycle (samples)
    cutoff_freq=500,   # Butterworth filter cutoff (Hz)
    scattering_j=6     # Scattering Transform depth
)
```

#### C√°c ph∆∞∆°ng th·ª©c ch√≠nh

##### 1. `process_dataset()`
X·ª≠ l√Ω to√†n b·ªô dataset t·ª´ raw files.

```python
X, y = preprocessor.process_dataset(
    raw_data_dir='../data/raw/training_data/',
    metadata_file='../data/raw/training_data.csv',
    use_scattering=True,  # True: Scattering features, False: raw signal
    verbose=True
)
```

**Output:**
- `X`: Features array (Scattering ho·∫∑c raw signal)
- `y`: Labels array (0=Absent, 1=Present)

##### 2. `balance_dataset()`
Balance dataset b·∫±ng downsampling.

```python
X_balanced, y_balanced = preprocessor.balance_dataset(X, y, random_state=42)
```

##### 3. `save_processed_data()` / `load_processed_data()`
L∆∞u v√† load d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω.

```python
# L∆∞u
preprocessor.save_processed_data(X, y, output_dir='../data/processed/', prefix='balanced')

# Load
X, y = preprocessor.load_processed_data(output_dir='../data/processed/', prefix='balanced')
```

##### 4. C√°c helper methods
- `butter_lowpass_filter()`: Butterworth low-pass filter
- `normalize_signal()`: Normalize signal (zero mean, unit variance)
- `extract_cycles()`: Tr√≠ch xu·∫•t cycles t·ª´ WAV + TSV
- `extract_scattering_features()`: Tr√≠ch xu·∫•t Scattering features
- `load_labels()`: Load labels t·ª´ metadata CSV

---

## 2. CNN Model (File g·ªëc)

### File: `04_training_cnn_method1 - Copy.ipynb`

### Architecture
```
Input (26, 47)
    ‚Üì
Conv1D(128) + LeakyReLU + MaxPooling + BatchNorm
    ‚Üì
Conv1D(64) + LeakyReLU + MaxPooling + BatchNorm
    ‚Üì
Flatten
    ‚Üì
Dense(128) + Dropout(0.3) + LeakyReLU
    ‚Üì
Dense(64) + Dropout(0.3) + LeakyReLU
    ‚Üì
Dense(32) + ReLU
    ‚Üì
Dense(2, softmax)
```

### K·∫øt qu·∫£
- **Cross Validation Mean Accuracy**: ~80.69%
- **Test Accuracy**: ~83.46%
- **Precision (Present)**: 88%
- **Recall (Present)**: 78%

### ƒê·∫∑c ƒëi·ªÉm
- ‚úÖ S·ª≠ d·ª•ng Scattering Transform features
- ‚úÖ 2 Conv blocks ƒë∆°n gi·∫£n
- ‚úÖ LeakyReLU activation
- ‚úÖ 5-fold Cross Validation

---

## 3. LSTM/GRU Model (File m·ªõi)

### File: `05_training_lstm_method.ipynb`

### Architecture Options

#### Option 1: Bidirectional LSTM
```
Input (26, 47)
    ‚Üì
Bidirectional LSTM(128) + BatchNorm + Dropout(0.3)
    ‚Üì
Bidirectional LSTM(64) + BatchNorm + Dropout(0.3)
    ‚Üì
GlobalAveragePooling1D
    ‚Üì
Dense(128) + Dropout(0.4) + BatchNorm
    ‚Üì
Dense(64) + Dropout(0.3)
    ‚Üì
Dense(2, softmax)
```

#### Option 2: Bidirectional GRU
```
Input (26, 47)
    ‚Üì
Bidirectional GRU(128) + BatchNorm + Dropout(0.3)
    ‚Üì
Bidirectional GRU(64) + BatchNorm + Dropout(0.3)
    ‚Üì
GlobalAveragePooling1D
    ‚Üì
Dense(128) + Dropout(0.4) + BatchNorm
    ‚Üì
Dense(64) + Dropout(0.3)
    ‚Üì
Dense(2, softmax)
```

### ƒê·∫∑c ƒëi·ªÉm
- ‚úÖ **Bidirectional**: H·ªçc features t·ª´ c·∫£ 2 h∆∞·ªõng (forward + backward)
- ‚úÖ **Global Average Pooling**: Thay v√¨ Flatten
- ‚úÖ **ReduceLROnPlateau**: T·ª± ƒë·ªông gi·∫£m learning rate
- ‚úÖ C√≥ th·ªÉ ch·ªçn LSTM ho·∫∑c GRU (GRU nhanh h∆°n)
- ‚úÖ S·ª≠ d·ª•ng preprocessing module

### ∆Øu ƒëi·ªÉm so v·ªõi CNN
- **LSTM/GRU** t·ªët h∆°n cho sequential data (time series)
- C√≥ kh·∫£ nƒÉng h·ªçc **long-term dependencies**
- **Bidirectional** gi√∫p context t·ª´ c·∫£ qu√° kh·ª© v√† t∆∞∆°ng lai

---

## 4. So s√°nh Models

| Model | Architecture | Params | Speed | Accuracy (d·ª± ki·∫øn) |
|-------|-------------|--------|-------|-------------------|
| **CNN** | 2 Conv blocks | ~Medium | Fast | ~83-84% |
| **LSTM** | 2 Bi-LSTM layers | ~High | Slower | ~84-86% |
| **GRU** | 2 Bi-GRU layers | ~Medium | Medium | ~83-85% |

### Khi n√†o d√πng model n√†o?

#### CNN
- ‚úÖ C·∫ßn inference nhanh
- ‚úÖ D·ªØ li·ªáu c√≥ local patterns r√µ r√†ng
- ‚úÖ T√†i nguy√™n h·∫°n ch·∫ø

#### LSTM
- ‚úÖ D·ªØ li·ªáu sequential ph·ª©c t·∫°p
- ‚úÖ C·∫ßn h·ªçc long-term dependencies
- ‚úÖ C√≥ ƒë·ªß t√†i nguy√™n training

#### GRU
- ‚úÖ C√¢n b·∫±ng gi·ªØa LSTM v√† CNN
- ‚úÖ Training nhanh h∆°n LSTM
- ‚úÖ Performance t∆∞∆°ng ƒë∆∞∆°ng LSTM

---

## 5. Workflow S·ª≠ D·ª•ng

### B∆∞·ªõc 1: Preprocessing (ch·ªâ ch·∫°y 1 l·∫ßn)
```python
from data_preprocessing import HeartSoundPreprocessor

# Kh·ªüi t·∫°o
preprocessor = HeartSoundPreprocessor(sr=4000, max_len=3000)

# X·ª≠ l√Ω data
X, y = preprocessor.process_dataset(raw_data_dir, metadata_file)

# Balance
X_balanced, y_balanced = preprocessor.balance_dataset(X, y)

# L∆∞u
preprocessor.save_processed_data(X_balanced, y_balanced, output_dir)
```

### B∆∞·ªõc 2: Train Model
```python
# Load data
X, y = preprocessor.load_processed_data(output_dir)

# Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Build model (CNN ho·∫∑c LSTM)
model = build_lstm_attention_model(X_train.shape[1:])

# Train
model.fit(X_train, y_train, validation_data=(X_test, y_test), ...)
```

---

## 6. Files Structure

```
VIP/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ data_preprocessing.py          # Module ti·ªÅn x·ª≠ l√Ω
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 04_training_cnn_method1 - Copy.ipynb   # CNN model (g·ªëc)
‚îÇ   ‚îú‚îÄ‚îÄ 05_training_lstm_method.ipynb          # LSTM/GRU model (m·ªõi)
‚îÇ   ‚îî‚îÄ‚îÄ example_using_preprocessor.ipynb       # Example usage
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ training_data/             # Raw WAV + TSV files
‚îÇ   ‚îú‚îÄ‚îÄ processed_cnn_method/          # Processed data cho CNN
‚îÇ   ‚îî‚îÄ‚îÄ processed_lstm_method/         # Processed data cho LSTM
‚îî‚îÄ‚îÄ results/
    ‚îî‚îÄ‚îÄ models/
        ‚îú‚îÄ‚îÄ 1dcnn_method_best.h5       # CNN model
        ‚îî‚îÄ‚îÄ lstm_model_best.h5         # LSTM model
```

---

## 7. Next Steps

### C·∫£i ti·∫øn c√≥ th·ªÉ th·ª±c hi·ªán:

1. **Ensemble Model**: K·∫øt h·ª£p CNN + LSTM
2. **Attention Mechanism**: Th√™m attention layer cho LSTM
3. **Data Augmentation**: TƒÉng c∆∞·ªùng d·ªØ li·ªáu
4. **Hyperparameter Tuning**: T·ªëi ∆∞u tham s·ªë
5. **Transfer Learning**: S·ª≠ d·ª•ng pretrained models
6. **Multi-task Learning**: Predict c·∫£ Murmur v√† Outcome

---

## 8. Requirements

```bash
# Core
numpy
pandas
librosa
scipy

# Deep Learning
tensorflow>=2.0
keras

# Scattering Transform
kymatio

# Visualization
matplotlib
seaborn

# Utils
scikit-learn
tqdm
```

---

## 9. Troubleshooting

### L·ªói: "Module not found: data_preprocessing"
```python
import sys
sys.path.append('../src')
from data_preprocessing import HeartSoundPreprocessor
```

### L·ªói: "Out of memory"
- Gi·∫£m `BATCH_SIZE`
- Gi·∫£m s·ªë units trong LSTM/GRU
- S·ª≠ d·ª•ng GRU thay v√¨ LSTM

### Model kh√¥ng converge
- TƒÉng `EARLY_STOPPING_PATIENCE`
- Th·ª≠ learning rate kh√°c
- Check data balance

---

## 10. Contact & Support

N·∫øu c√≥ v·∫•n ƒë·ªÅ ho·∫∑c c√¢u h·ªèi, vui l√≤ng:
1. Check documentation n√†y
2. Xem example notebook
3. Review code comments trong module

**Happy Coding! üöÄ**
